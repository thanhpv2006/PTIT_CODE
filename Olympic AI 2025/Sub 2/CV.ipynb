{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Ych9BDcwBiZ2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import torch.nn as nn\n",
        "import warnings\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import time, torch\n",
        "from tqdm import tqdm\n",
        "warnings.filterwarnings('ignore')\n",
        "import os\n",
        "import random\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms, datasets\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "root = \"/content/data/\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fecq96BALPY-",
        "outputId": "f2b8fe53-4604-4523-d7a1-951c737cc743"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.8.0+cu126\n",
            "CUDA available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SHARED_DATA_DIR =  \"/content/drive/MyDrive/Olympic AI/CV\"\n",
        "LOCAL_DATA_DIR = \"/content/data\"\n",
        "\n",
        "!mkdir -p \"/content/data\" \"$LOCAL_DATA_DIR\"\n",
        "\n",
        "!rsync -ah --progress \"$SHARED_DATA_DIR\"/ \"$LOCAL_DATA_DIR\"/\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_zt_mqKELrV",
        "outputId": "8826e8bd-b654-4b30-c330-f0636d6ac708"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sending incremental file list\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Import thÆ° viá»‡n\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "from tqdm import tqdm # Äá»ƒ hiá»ƒn thá»‹ tiáº¿n Ä‘á»™\n",
        "from sklearn.metrics import f1_score # Äá»ƒ tÃ­nh toÃ¡n F1-score\n",
        "import time # Äá»ƒ Ä‘o thá»i gian\n",
        "\n",
        "# --- ÄÆ°á»ng dáº«n thÆ° má»¥c gá»‘c cá»§a báº¡n ---\n",
        "# Vui lÃ²ng Ä‘iá»u chá»‰nh biáº¿n 'root' nÃ y Ä‘á»ƒ trá» Ä‘áº¿n thÆ° má»¥c chá»©a 'dataset' cá»§a báº¡n.\n",
        "# VÃ­ dá»¥: náº¿u cáº¥u trÃºc lÃ  /path/to/your/project/dataset/train/images,\n",
        "# thÃ¬ root = '/path/to/your/project'\n",
        "\n",
        "# 2. Thiáº¿t láº­p Seed Ä‘á»ƒ tÃ¡i láº­p káº¿t quáº£\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        # CÃ i Ä‘áº·t nÃ y cÃ³ thá»ƒ lÃ m cháº­m quÃ¡ trÃ¬nh huáº¥n luyá»‡n nhÆ°ng Ä‘áº£m báº£o tÃ¡i láº­p káº¿t quáº£ trÃªn GPU\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    print(f\"ÄÃ£ thiáº¿t láº­p seed = {seed}\")\n",
        "\n",
        "SEED = 42\n",
        "set_seed(SEED)\n",
        "\n",
        "# 3. Äá»‹nh nghÄ©a Transform cho áº£nh\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)), # ResNet18 thÆ°á»ng nháº­n Ä‘áº§u vÃ o 224x224\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406], # Mean vÃ  Std chuáº©n cá»§a ImageNet\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    ),\n",
        "])\n",
        "print(\"ÄÃ£ Ä‘á»‹nh nghÄ©a transform\")\n",
        "\n",
        "# 4. XÃ¢y dá»±ng Dataset Class\n",
        "class StormDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, is_test=False):\n",
        "        self.root_dir = root_dir\n",
        "        self.img_dir = os.path.join(root_dir, \"images\")\n",
        "        self.transform = transform\n",
        "        self.is_test = is_test\n",
        "\n",
        "        if is_test:\n",
        "            # Äá»‘i vá»›i táº­p test, khÃ´ng cÃ³ nhÃ£n. annotations.csv chá»‰ chá»©a file_name.\n",
        "            # Hoáº·c náº¿u khÃ´ng cÃ³ annotations.csv, chá»‰ cáº§n Ä‘á»c danh sÃ¡ch áº£nh.\n",
        "            # Äá»ƒ an toÃ n, chÃºng ta sáº½ táº¡o má»™t DataFrame tá»« cÃ¡c tÃªn file áº£nh.\n",
        "            self.image_files = sorted([f for f in os.listdir(self.img_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "            self.annotations = pd.DataFrame({'file_name': self.image_files})\n",
        "        else:\n",
        "            # Äá»‘i vá»›i táº­p train/val, cáº§n file annotations.csv cÃ³ nhÃ£n.\n",
        "            annotations_path = os.path.join(root_dir, \"annotations.csv\")\n",
        "            if not os.path.exists(annotations_path):\n",
        "                raise FileNotFoundError(f\"File annotations.csv khÃ´ng tÃ¬m tháº¥y táº¡i: {annotations_path}\")\n",
        "            self.annotations = pd.read_csv(annotations_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.annotations.iloc[idx]\n",
        "        img_path = os.path.join(self.img_dir, row['file_name'])\n",
        "\n",
        "        # Kiá»ƒm tra xem file áº£nh cÃ³ tá»“n táº¡i khÃ´ng\n",
        "        if not os.path.exists(img_path):\n",
        "            raise FileNotFoundError(f\"File áº£nh khÃ´ng tÃ¬m tháº¥y táº¡i: {img_path}\")\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Logic nhÃ£n:\n",
        "        # Náº¿u lÃ  táº­p test, nhÃ£n máº·c Ä‘á»‹nh lÃ  -1 (hoáº·c má»™t giÃ¡ trá»‹ khÃ´ng há»£p lá»‡ khÃ¡c)\n",
        "        # hoáº·c cÃ³ thá»ƒ bá» qua tráº£ vá» nhÃ£n. á»ž Ä‘Ã¢y giá»¯ -1 Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch vá»›i pipeline.\n",
        "        if self.is_test:\n",
        "            # DÃ¹ cÃ³ cá»™t 'is_negative' hay 'category_id' trong file annotations.csv cá»§a test set,\n",
        "            # chÃºng ta khÃ´ng sá»­ dá»¥ng chÃºng Ä‘á»ƒ huáº¥n luyá»‡n.\n",
        "            # GiÃ¡ trá»‹ tráº£ vá» nhÃ£n sáº½ lÃ  -1 Ä‘á»ƒ bÃ¡o hiá»‡u Ä‘Ã¢y lÃ  táº­p test.\n",
        "            return image, -1\n",
        "        else:\n",
        "            # Äá»‘i vá»›i táº­p train/val, Ä‘á»c nhÃ£n tá»« annotations.csv\n",
        "            # is_negative: True -> label 0\n",
        "            # category_id: 1, 2, 3, 4 -> label 1, 2, 3, 4\n",
        "            # Tá»•ng cá»™ng cÃ³ 5 lá»›p (0, 1, 2, 3, 4)\n",
        "            if row['is_negative']:\n",
        "                label = 0\n",
        "            else:\n",
        "                label = int(row['category_id'])\n",
        "            return image, label\n",
        "\n",
        "print(\"ÄÃ£ Ä‘á»‹nh nghÄ©a StormDataset\")\n",
        "\n",
        "\n",
        "# 5. XÃ¢y dá»±ng Model CNN (ResNet18) - Thay tháº¿ SimpleCNN\n",
        "# ------------------------------------------------------------------------------------\n",
        "# Äá»‹nh nghÄ©a BasicBlock cho ResNet18/34\n",
        "class BasicBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Khá»‘i xÃ¢y dá»±ng cÆ¡ báº£n cá»§a ResNet cho cÃ¡c phiÃªn báº£n nhÆ° ResNet18 vÃ  ResNet34.\n",
        "    Bao gá»“m hai lá»›p tÃ­ch cháº­p 3x3 vÃ  má»™t káº¿t ná»‘i táº¯t (shortcut connection).\n",
        "    \"\"\"\n",
        "    expansion = 1 # Há»‡ sá»‘ má»Ÿ rá»™ng sá»‘ kÃªnh Ä‘áº§u ra. Vá»›i BasicBlock, sá»‘ kÃªnh khÃ´ng Ä‘á»•i.\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        # Lá»›p tÃ­ch cháº­p Ä‘áº§u tiÃªn trong block\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Lá»›p tÃ­ch cháº­p thá»© hai trong block\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels * self.expansion)\n",
        "\n",
        "        # Káº¿t ná»‘i táº¯t (shortcut connection)\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels * self.expansion:\n",
        "            # Náº¿u kÃ­ch thÆ°á»›c khÃ´ng gian (stride != 1) hoáº·c sá»‘ kÃªnh thay Ä‘á»•i,\n",
        "            # cáº§n má»™t phÃ©p tÃ­ch cháº­p 1x1 trÃªn nhÃ¡nh táº¯t Ä‘á»ƒ khá»›p kÃ­ch thÆ°á»›c.\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels * self.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * self.expansion)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x # LÆ°u trá»¯ Ä‘áº§u vÃ o cho káº¿t ná»‘i táº¯t\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out += self.shortcut(identity) # Cá»™ng nhÃ¡nh chÃ­nh vá»›i nhÃ¡nh táº¯t (residual connection)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "# Äá»‹nh nghÄ©a kiáº¿n trÃºc ResNet tá»•ng quÃ¡t\n",
        "class ResNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Kiáº¿n trÃºc ResNet tá»•ng quÃ¡t.\n",
        "    Sá»­ dá»¥ng cÃ¡c BasicBlock (hoáº·c BottleneckBlock cho cÃ¡c phiÃªn báº£n sÃ¢u hÆ¡n)\n",
        "    Ä‘á»ƒ xÃ¢y dá»±ng mÃ´ hÃ¬nh.\n",
        "    \"\"\"\n",
        "    def __init__(self, block, num_blocks, num_classes=5, in_channels=3):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64 # Sá»‘ kÃªnh Ä‘áº§u ra sau lá»›p conv1\n",
        "\n",
        "        # Lá»›p tÃ­ch cháº­p khá»Ÿi táº¡o: ThÆ°á»ng lÃ  kernel_size=7, stride=2, MaxPool\n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # CÃ¡c táº§ng ResNet (layer stages)\n",
        "        # Má»—i táº§ng chá»©a má»™t sá»‘ lÆ°á»£ng block nháº¥t Ä‘á»‹nh, vÃ  táº§ng Ä‘áº§u tiÃªn cá»§a má»—i nhÃ³m (trá»« layer1)\n",
        "        # sáº½ thá»±c hiá»‡n downsampling (stride=2)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "\n",
        "        # Lá»›p Global Average Pooling Ä‘á»ƒ giáº£m kÃ­ch thÆ°á»›c khÃ´ng gian xuá»‘ng 1x1\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # Lá»›p Fully Connected cuá»‘i cÃ¹ng Ä‘á»ƒ phÃ¢n loáº¡i\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        # Khá»Ÿi táº¡o trá»ng sá»‘ (quan trá»ng khi train tá»« scratch)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, layer_block, out_channels, num_blocks, stride):\n",
        "        \"\"\"\n",
        "        HÃ m trá»£ giÃºp Ä‘á»ƒ táº¡o má»™t táº§ng (stage) cá»§a ResNet.\n",
        "        Trong má»™t táº§ng, block Ä‘áº§u tiÃªn cÃ³ thá»ƒ cÃ³ stride > 1 Ä‘á»ƒ downsample.\n",
        "        \"\"\"\n",
        "        strides = [stride] + [1] * (num_blocks - 1) # stride chá»‰ Ã¡p dá»¥ng cho block Ä‘áº§u tiÃªn\n",
        "        layers = []\n",
        "        for s in strides:\n",
        "            layers.append(layer_block(self.in_channels, out_channels, s))\n",
        "            self.in_channels = out_channels * layer_block.expansion # Cáº­p nháº­t in_channels cho block tiáº¿p theo\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass qua lá»›p khá»Ÿi táº¡o\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        # Forward pass qua cÃ¡c táº§ng ResNet\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        # Forward pass qua Global Average Pooling vÃ  lá»›p Fully Connected\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1) # LÃ m pháº³ng Ä‘áº§u ra thÃ nh vector\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# HÃ m táº¡o ResNet18 cá»¥ thá»ƒ\n",
        "def ResNet18(num_classes=5, in_channels=3):\n",
        "    \"\"\"\n",
        "    HÃ m khá»Ÿi táº¡o mÃ´ hÃ¬nh ResNet18.\n",
        "    Sá»­ dá»¥ng BasicBlock vÃ  cáº¥u hÃ¬nh sá»‘ lÆ°á»£ng block cho má»—i táº§ng: [2, 2, 2, 2].\n",
        "    \"\"\"\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, in_channels)\n",
        "\n",
        "# ------------------------------------------------------------------------------------\n",
        "\n",
        "# Khá»Ÿi táº¡o ResNet18\n",
        "# num_classes = 5 (0, 1, 2, 3, 4) vÃ¬ cÃ³ cáº£ lá»›p 'is_negative' (0) vÃ  4 cáº¥p Ä‘á»™ bÃ£o (1-4)\n",
        "model = ResNet18(num_classes=5, in_channels=3)\n",
        "print(f\"ÄÃ£ xÃ¢y dá»±ng mÃ´ hÃ¬nh ResNet18\")\n",
        "print(f\"Tá»•ng sá»‘ parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "\n",
        "# 6. Chuáº©n bá»‹ DataLoader\n",
        "def worker_init_fn(worker_id):\n",
        "    \"\"\"Äáº£m báº£o tÃ¡i láº­p káº¿t quáº£ cho cÃ¡c worker cá»§a DataLoader.\"\"\"\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "full_dataset = StormDataset(os.path.join(root, \"dataset/train\"), transform=transform)\n",
        "total_size = len(full_dataset)\n",
        "train_size = int(0.8 * total_size)\n",
        "val_size = total_size - train_size\n",
        "\n",
        "print(f\"Tá»•ng sá»‘ áº£nh huáº¥n luyá»‡n/validation: {total_size}\")\n",
        "print(f\"Táº­p huáº¥n luyá»‡n: {train_size}, Táº­p validation: {val_size}\")\n",
        "\n",
        "generator = torch.Generator().manual_seed(SEED) # DÃ¹ng cho random_split vÃ  DataLoader workers\n",
        "train_dataset, val_dataset = random_split(\n",
        "    full_dataset, [train_size, val_size], generator=generator\n",
        ")\n",
        "\n",
        "BATCH_SIZE = 64 # CÃ³ thá»ƒ Ä‘iá»u chá»‰nh Batch Size\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "    worker_init_fn=worker_init_fn, generator=generator, # generator cho shuffle vÃ  worker_init_fn\n",
        "    num_workers=4, # TÄƒng num_workers náº¿u CPU/RAM cho phÃ©p Ä‘á»ƒ tÄƒng tá»‘c load dá»¯ liá»‡u\n",
        "    pin_memory=True # GiÃºp tÄƒng tá»‘c Ä‘á»™ chuyá»ƒn dá»¯ liá»‡u lÃªn GPU\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "    worker_init_fn=worker_init_fn, # Äáº£m báº£o tÃ¡i láº­p káº¿t quáº£\n",
        "    num_workers=4,\n",
        "    pin_memory=True\n",
        ")\n",
        "print(f\"ÄÃ£ táº¡o DataLoader (batch_size={BATCH_SIZE})\")\n",
        "\n",
        "\n",
        "# 7. Train Model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"ðŸ”§ Sá»­ dá»¥ng device: {device}\")\n",
        "\n",
        "model.to(device) # Chuyá»ƒn mÃ´ hÃ¬nh sang thiáº¿t bá»‹ tÃ­nh toÃ¡n\n",
        "\n",
        "# Criterion (HÃ m máº¥t mÃ¡t) vÃ  Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Sá»­ dá»¥ng AdamW thay vÃ¬ Adam thÆ°á»ng Ä‘á»ƒ cÃ³ kháº£ nÄƒng Ä‘iá»u chá»‰nh Weight Decay tá»‘t hÆ¡n\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4) # Giáº£m LR vÃ  thÃªm Weight Decay\n",
        "\n",
        "# Learning Rate Scheduler (giÃºp cáº£i thiá»‡n hiá»‡u suáº¥t, Ä‘áº·c biá»‡t vá»›i cÃ¡c máº¡ng sÃ¢u)\n",
        "# Giáº£m learning rate khi validation loss ngá»«ng cáº£i thiá»‡n\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
        "\n",
        "NUM_EPOCHS = 25 # TÄƒng sá»‘ epoch Ä‘á»ƒ ResNet18 cÃ³ Ä‘á»§ thá»i gian há»c tá»« scratch\n",
        "print(f\"Báº¯t Ä‘áº§u huáº¥n luyá»‡n ({NUM_EPOCHS} epochs)...\")\n",
        "\n",
        "history = {\n",
        "    'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'val_f1': []\n",
        "}\n",
        "best_val_f1 = -1.0\n",
        "best_epoch = -1\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    start_epoch_time = time.time()\n",
        "\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    train_loss = running_loss / total\n",
        "    train_acc = 100 * correct / total\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    val_correct, val_total = 0, 0\n",
        "    val_running_loss = 0.0\n",
        "    all_val_labels = []\n",
        "    all_val_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Val]\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "\n",
        "            all_val_labels.extend(labels.cpu().numpy())\n",
        "            all_val_preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "    val_loss = val_running_loss / val_total\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "\n",
        "    # TÃ­nh F1-score trÃªn táº­p validation\n",
        "    # average='weighted' lÃ  lá»±a chá»n tá»‘t cho cÃ¡c táº­p dá»¯ liá»‡u khÃ´ng cÃ¢n báº±ng\n",
        "    val_f1 = f1_score(all_val_labels, all_val_preds, average='weighted')\n",
        "\n",
        "    # Cáº­p nháº­t Learning Rate Scheduler\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    history['val_f1'].append(val_f1)\n",
        "\n",
        "    epoch_duration = time.time() - start_epoch_time\n",
        "\n",
        "    print(f\"\\nEpoch [{epoch+1}/{NUM_EPOCHS}] ({epoch_duration:.2f}s):\")\n",
        "    print(f\" Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "    print(f\" Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, Val F1: {val_f1:.4f}\")\n",
        "\n",
        "    # LÆ°u model tá»‘t nháº¥t dá»±a trÃªn Val F1-score\n",
        "    if val_f1 > best_val_f1:\n",
        "        best_val_f1 = val_f1\n",
        "        best_epoch = epoch + 1\n",
        "        model_path = os.path.join(root, \"resnet18_best_f1_model.pth\")\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "        print(f\"  >>> LÆ°u model tá»‘t nháº¥t vá»›i Val F1: {best_val_f1:.4f} táº¡i Epoch {best_epoch} <<<\")\n",
        "\n",
        "print(\"\\nHoÃ n thÃ nh huáº¥n luyá»‡n!\")\n",
        "print(f\"Model tá»‘t nháº¥t Ä‘áº¡t Val F1 = {best_val_f1:.4f} táº¡i Epoch {best_epoch}\")\n",
        "\n",
        "# 8. LÆ°u Model cuá»‘i cÃ¹ng vÃ  lá»‹ch sá»­ huáº¥n luyá»‡n\n",
        "# (Chá»‰ lÆ°u model tá»‘t nháº¥t trong vÃ²ng láº·p)\n",
        "# model_path = os.path.join(root, \"resnet18_final_model.pth\")\n",
        "# torch.save(model.state_dict(), model_path)\n",
        "# print(f\"ÄÃ£ lÆ°u model cuá»‘i cÃ¹ng táº¡i: {model_path}\")\n",
        "\n",
        "history_path = os.path.join(root, \"training_history_resnet18.csv\")\n",
        "history_df = pd.DataFrame(history)\n",
        "history_df.to_csv(history_path, index=False)\n",
        "print(f\"ÄÃ£ lÆ°u lá»‹ch sá»­ huáº¥n luyá»‡n táº¡i: {history_path}\")\n",
        "\n",
        "\n",
        "# --- Táº£i láº¡i model tá»‘t nháº¥t Ä‘á»ƒ suy luáº­n ---\n",
        "print(\"\\nÄang táº£i láº¡i model tá»‘t nháº¥t Ä‘á»ƒ suy luáº­n...\")\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.to(device)\n",
        "print(f\"ÄÃ£ táº£i model tá»« '{model_path}'\")\n",
        "\n",
        "\n",
        "# 9. Inference trÃªn Public Test Set\n",
        "print(\"\\nBáº¯t Ä‘áº§u inference trÃªn PUBLIC TEST SET...\")\n",
        "public_test_dataset = StormDataset(os.path.join(root, \"dataset/public_test\"), transform=transform, is_test=True)\n",
        "public_test_loader = DataLoader(\n",
        "    public_test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True\n",
        ")\n",
        "\n",
        "model.eval() # Chuyá»ƒn model sang cháº¿ Ä‘á»™ Ä‘Ã¡nh giÃ¡\n",
        "file_names = []\n",
        "preds_is_negative = []\n",
        "preds_category_id = []\n",
        "\n",
        "with torch.no_grad(): # KhÃ´ng tÃ­nh toÃ¡n gradient trong quÃ¡ trÃ¬nh suy luáº­n\n",
        "    for i, (images, _) in enumerate(tqdm(public_test_loader, desc=\"Public Test\")):\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = outputs.max(1) # Láº¥y lá»›p cÃ³ Ä‘iá»ƒm sá»‘ cao nháº¥t\n",
        "\n",
        "        # Láº¥y tÃªn file tÆ°Æ¡ng á»©ng tá»« DataFrame cá»§a Dataset\n",
        "        batch_start = i * BATCH_SIZE\n",
        "        batch_end = min(batch_start + images.size(0), len(public_test_dataset.annotations)) # Äáº£m báº£o khÃ´ng vÆ°á»£t quÃ¡ kÃ­ch thÆ°á»›c dataset\n",
        "        batch_files = public_test_dataset.annotations.iloc[batch_start:batch_end]['file_name'].values\n",
        "        file_names.extend(batch_files)\n",
        "\n",
        "        for p in predicted.cpu().numpy():\n",
        "            if p == 0: # Lá»›p 0 tÆ°Æ¡ng á»©ng vá»›i is_negative = True\n",
        "                preds_is_negative.append(True)\n",
        "                preds_category_id.append(\"\") # category_id Ä‘á»ƒ trá»‘ng náº¿u is_negative lÃ  True\n",
        "            else: # CÃ¡c lá»›p 1-4 tÆ°Æ¡ng á»©ng vá»›i category_id\n",
        "                preds_is_negative.append(False)\n",
        "                preds_category_id.append(int(p)) # Chuyá»ƒn vá» int Ä‘á»ƒ lÆ°u\n",
        "\n",
        "df_public = pd.DataFrame({\n",
        "    'file_name': file_names,\n",
        "    'is_negative': preds_is_negative,\n",
        "    'category_id': preds_category_id\n",
        "})\n",
        "\n",
        "public_csv_path = os.path.join(root, \"public_cv.csv\")\n",
        "df_public.to_csv(public_csv_path, index=False)\n",
        "print(f\"ÄÃ£ táº¡o file public_cv.csv táº¡i {public_csv_path} vá»›i {len(df_public)} dá»± Ä‘oÃ¡n!\")\n",
        "print(\"\\nPreview káº¿t quáº£ Public Test:\")\n",
        "print(df_public.head(10))\n",
        "\n",
        "\n",
        "# 10. Inference trÃªn Private Test Set\n",
        "print(\"\\nBáº¯t Ä‘áº§u inference trÃªn PRIVATE TEST SET...\")\n",
        "private_test_dataset = StormDataset(os.path.join(root, \"dataset/private_test\"), transform=transform, is_test=True)\n",
        "private_test_loader = DataLoader(\n",
        "    private_test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True\n",
        ")\n",
        "\n",
        "model.eval() # Chuyá»ƒn model sang cháº¿ Ä‘á»™ Ä‘Ã¡nh giÃ¡\n",
        "file_names = []\n",
        "preds_is_negative = []\n",
        "preds_category_id = []\n",
        "\n",
        "with torch.no_grad(): # KhÃ´ng tÃ­nh toÃ¡n gradient trong quÃ¡ trÃ¬nh suy luáº­n\n",
        "    for i, (images, _) in enumerate(tqdm(private_test_loader, desc=\"Private Test\")):\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = outputs.max(1) # Láº¥y lá»›p cÃ³ Ä‘iá»ƒm sá»‘ cao nháº¥t\n",
        "\n",
        "        # Láº¥y tÃªn file tÆ°Æ¡ng á»©ng tá»« DataFrame cá»§a Dataset\n",
        "        batch_start = i * BATCH_SIZE\n",
        "        batch_end = min(batch_start + images.size(0), len(private_test_dataset.annotations)) # Äáº£m báº£o khÃ´ng vÆ°á»£t quÃ¡ kÃ­ch thÆ°á»›c dataset\n",
        "        batch_files = private_test_dataset.annotations.iloc[batch_start:batch_end]['file_name'].values\n",
        "        file_names.extend(batch_files)\n",
        "\n",
        "        for p in predicted.cpu().numpy():\n",
        "            if p == 0: # Lá»›p 0 tÆ°Æ¡ng á»©ng vá»›i is_negative = True\n",
        "                preds_is_negative.append(True)\n",
        "                preds_category_id.append(\"\") # category_id Ä‘á»ƒ trá»‘ng náº¿u is_negative lÃ  True\n",
        "            else: # CÃ¡c lá»›p 1-4 tÆ°Æ¡ng á»©ng vá»›i category_id\n",
        "                preds_is_negative.append(False)\n",
        "                preds_category_id.append(int(p)) # Chuyá»ƒn vá» int Ä‘á»ƒ lÆ°u\n",
        "\n",
        "df_private = pd.DataFrame({\n",
        "    'file_name': file_names,\n",
        "    'is_negative': preds_is_negative,\n",
        "    'category_id': preds_category_id\n",
        "})\n",
        "\n",
        "private_csv_path = os.path.join(root, \"private_cv.csv\")\n",
        "df_private.to_csv(private_csv_path, index=False)\n",
        "print(f\"ÄÃ£ táº¡o file private_cv.csv vá»›i {len(df_private)} dá»± Ä‘oÃ¡n!\")\n",
        "print(\"\\nPreview káº¿t quáº£ Private Test:\")\n",
        "print(df_private.head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VA4x9bT5DXjQ",
        "outputId": "c42a0b55-819d-4e59-aee5-a6d9dfbb5b6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ÄÃ£ thiáº¿t láº­p seed = 42\n",
            "ÄÃ£ Ä‘á»‹nh nghÄ©a transform\n",
            "ÄÃ£ Ä‘á»‹nh nghÄ©a StormDataset\n",
            "ÄÃ£ xÃ¢y dá»±ng mÃ´ hÃ¬nh ResNet18\n",
            "Tá»•ng sá»‘ parameters: 11,179,077\n",
            "Tá»•ng sá»‘ áº£nh huáº¥n luyá»‡n/validation: 8385\n",
            "Táº­p huáº¥n luyá»‡n: 6708, Táº­p validation: 1677\n",
            "ÄÃ£ táº¡o DataLoader (batch_size=64)\n",
            "ðŸ”§ Sá»­ dá»¥ng device: cuda\n",
            "Báº¯t Ä‘áº§u huáº¥n luyá»‡n (25 epochs)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/25 [Train]:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 60/105 [00:44<00:22,  2.02it/s]"
          ]
        }
      ]
    }
  ]
}