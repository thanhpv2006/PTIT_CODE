{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Ych9BDcwBiZ2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import torch.nn as nn\n",
        "import warnings\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import time, torch\n",
        "from tqdm import tqdm\n",
        "warnings.filterwarnings('ignore')\n",
        "import os\n",
        "import random\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms, datasets\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "root = \"/content/data/\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fecq96BALPY-",
        "outputId": "f2b8fe53-4604-4523-d7a1-951c737cc743"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.8.0+cu126\n",
            "CUDA available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SHARED_DATA_DIR =  \"/content/drive/MyDrive/Olympic AI/CV\"\n",
        "LOCAL_DATA_DIR = \"/content/data\"\n",
        "\n",
        "!mkdir -p \"/content/data\" \"$LOCAL_DATA_DIR\"\n",
        "\n",
        "!rsync -ah --progress \"$SHARED_DATA_DIR\"/ \"$LOCAL_DATA_DIR\"/\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_zt_mqKELrV",
        "outputId": "8826e8bd-b654-4b30-c330-f0636d6ac708"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sending incremental file list\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Import thư viện\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "from tqdm import tqdm # Để hiển thị tiến độ\n",
        "from sklearn.metrics import f1_score # Để tính toán F1-score\n",
        "import time # Để đo thời gian\n",
        "\n",
        "# --- Đường dẫn thư mục gốc của bạn ---\n",
        "# Vui lòng điều chỉnh biến 'root' này để trỏ đến thư mục chứa 'dataset' của bạn.\n",
        "# Ví dụ: nếu cấu trúc là /path/to/your/project/dataset/train/images,\n",
        "# thì root = '/path/to/your/project'\n",
        "\n",
        "# 2. Thiết lập Seed để tái lập kết quả\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        # Cài đặt này có thể làm chậm quá trình huấn luyện nhưng đảm bảo tái lập kết quả trên GPU\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    print(f\"Đã thiết lập seed = {seed}\")\n",
        "\n",
        "SEED = 42\n",
        "set_seed(SEED)\n",
        "\n",
        "# 3. Định nghĩa Transform cho ảnh\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)), # ResNet18 thường nhận đầu vào 224x224\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406], # Mean và Std chuẩn của ImageNet\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    ),\n",
        "])\n",
        "print(\"Đã định nghĩa transform\")\n",
        "\n",
        "# 4. Xây dựng Dataset Class\n",
        "class StormDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, is_test=False):\n",
        "        self.root_dir = root_dir\n",
        "        self.img_dir = os.path.join(root_dir, \"images\")\n",
        "        self.transform = transform\n",
        "        self.is_test = is_test\n",
        "\n",
        "        if is_test:\n",
        "            # Đối với tập test, không có nhãn. annotations.csv chỉ chứa file_name.\n",
        "            # Hoặc nếu không có annotations.csv, chỉ cần đọc danh sách ảnh.\n",
        "            # Để an toàn, chúng ta sẽ tạo một DataFrame từ các tên file ảnh.\n",
        "            self.image_files = sorted([f for f in os.listdir(self.img_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "            self.annotations = pd.DataFrame({'file_name': self.image_files})\n",
        "        else:\n",
        "            # Đối với tập train/val, cần file annotations.csv có nhãn.\n",
        "            annotations_path = os.path.join(root_dir, \"annotations.csv\")\n",
        "            if not os.path.exists(annotations_path):\n",
        "                raise FileNotFoundError(f\"File annotations.csv không tìm thấy tại: {annotations_path}\")\n",
        "            self.annotations = pd.read_csv(annotations_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.annotations.iloc[idx]\n",
        "        img_path = os.path.join(self.img_dir, row['file_name'])\n",
        "\n",
        "        # Kiểm tra xem file ảnh có tồn tại không\n",
        "        if not os.path.exists(img_path):\n",
        "            raise FileNotFoundError(f\"File ảnh không tìm thấy tại: {img_path}\")\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Logic nhãn:\n",
        "        # Nếu là tập test, nhãn mặc định là -1 (hoặc một giá trị không hợp lệ khác)\n",
        "        # hoặc có thể bỏ qua trả về nhãn. Ở đây giữ -1 để tương thích với pipeline.\n",
        "        if self.is_test:\n",
        "            # Dù có cột 'is_negative' hay 'category_id' trong file annotations.csv của test set,\n",
        "            # chúng ta không sử dụng chúng để huấn luyện.\n",
        "            # Giá trị trả về nhãn sẽ là -1 để báo hiệu đây là tập test.\n",
        "            return image, -1\n",
        "        else:\n",
        "            # Đối với tập train/val, đọc nhãn từ annotations.csv\n",
        "            # is_negative: True -> label 0\n",
        "            # category_id: 1, 2, 3, 4 -> label 1, 2, 3, 4\n",
        "            # Tổng cộng có 5 lớp (0, 1, 2, 3, 4)\n",
        "            if row['is_negative']:\n",
        "                label = 0\n",
        "            else:\n",
        "                label = int(row['category_id'])\n",
        "            return image, label\n",
        "\n",
        "print(\"Đã định nghĩa StormDataset\")\n",
        "\n",
        "\n",
        "# 5. Xây dựng Model CNN (ResNet18) - Thay thế SimpleCNN\n",
        "# ------------------------------------------------------------------------------------\n",
        "# Định nghĩa BasicBlock cho ResNet18/34\n",
        "class BasicBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Khối xây dựng cơ bản của ResNet cho các phiên bản như ResNet18 và ResNet34.\n",
        "    Bao gồm hai lớp tích chập 3x3 và một kết nối tắt (shortcut connection).\n",
        "    \"\"\"\n",
        "    expansion = 1 # Hệ số mở rộng số kênh đầu ra. Với BasicBlock, số kênh không đổi.\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        # Lớp tích chập đầu tiên trong block\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Lớp tích chập thứ hai trong block\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels * self.expansion)\n",
        "\n",
        "        # Kết nối tắt (shortcut connection)\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels * self.expansion:\n",
        "            # Nếu kích thước không gian (stride != 1) hoặc số kênh thay đổi,\n",
        "            # cần một phép tích chập 1x1 trên nhánh tắt để khớp kích thước.\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels * self.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * self.expansion)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x # Lưu trữ đầu vào cho kết nối tắt\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out += self.shortcut(identity) # Cộng nhánh chính với nhánh tắt (residual connection)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "# Định nghĩa kiến trúc ResNet tổng quát\n",
        "class ResNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Kiến trúc ResNet tổng quát.\n",
        "    Sử dụng các BasicBlock (hoặc BottleneckBlock cho các phiên bản sâu hơn)\n",
        "    để xây dựng mô hình.\n",
        "    \"\"\"\n",
        "    def __init__(self, block, num_blocks, num_classes=5, in_channels=3):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64 # Số kênh đầu ra sau lớp conv1\n",
        "\n",
        "        # Lớp tích chập khởi tạo: Thường là kernel_size=7, stride=2, MaxPool\n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Các tầng ResNet (layer stages)\n",
        "        # Mỗi tầng chứa một số lượng block nhất định, và tầng đầu tiên của mỗi nhóm (trừ layer1)\n",
        "        # sẽ thực hiện downsampling (stride=2)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "\n",
        "        # Lớp Global Average Pooling để giảm kích thước không gian xuống 1x1\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # Lớp Fully Connected cuối cùng để phân loại\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        # Khởi tạo trọng số (quan trọng khi train từ scratch)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, layer_block, out_channels, num_blocks, stride):\n",
        "        \"\"\"\n",
        "        Hàm trợ giúp để tạo một tầng (stage) của ResNet.\n",
        "        Trong một tầng, block đầu tiên có thể có stride > 1 để downsample.\n",
        "        \"\"\"\n",
        "        strides = [stride] + [1] * (num_blocks - 1) # stride chỉ áp dụng cho block đầu tiên\n",
        "        layers = []\n",
        "        for s in strides:\n",
        "            layers.append(layer_block(self.in_channels, out_channels, s))\n",
        "            self.in_channels = out_channels * layer_block.expansion # Cập nhật in_channels cho block tiếp theo\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass qua lớp khởi tạo\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        # Forward pass qua các tầng ResNet\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        # Forward pass qua Global Average Pooling và lớp Fully Connected\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1) # Làm phẳng đầu ra thành vector\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Hàm tạo ResNet18 cụ thể\n",
        "def ResNet18(num_classes=5, in_channels=3):\n",
        "    \"\"\"\n",
        "    Hàm khởi tạo mô hình ResNet18.\n",
        "    Sử dụng BasicBlock và cấu hình số lượng block cho mỗi tầng: [2, 2, 2, 2].\n",
        "    \"\"\"\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, in_channels)\n",
        "\n",
        "# ------------------------------------------------------------------------------------\n",
        "\n",
        "# Khởi tạo ResNet18\n",
        "# num_classes = 5 (0, 1, 2, 3, 4) vì có cả lớp 'is_negative' (0) và 4 cấp độ bão (1-4)\n",
        "model = ResNet18(num_classes=5, in_channels=3)\n",
        "print(f\"Đã xây dựng mô hình ResNet18\")\n",
        "print(f\"Tổng số parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "\n",
        "# 6. Chuẩn bị DataLoader\n",
        "def worker_init_fn(worker_id):\n",
        "    \"\"\"Đảm bảo tái lập kết quả cho các worker của DataLoader.\"\"\"\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "full_dataset = StormDataset(os.path.join(root, \"dataset/train\"), transform=transform)\n",
        "total_size = len(full_dataset)\n",
        "train_size = int(0.8 * total_size)\n",
        "val_size = total_size - train_size\n",
        "\n",
        "print(f\"Tổng số ảnh huấn luyện/validation: {total_size}\")\n",
        "print(f\"Tập huấn luyện: {train_size}, Tập validation: {val_size}\")\n",
        "\n",
        "generator = torch.Generator().manual_seed(SEED) # Dùng cho random_split và DataLoader workers\n",
        "train_dataset, val_dataset = random_split(\n",
        "    full_dataset, [train_size, val_size], generator=generator\n",
        ")\n",
        "\n",
        "BATCH_SIZE = 64 # Có thể điều chỉnh Batch Size\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "    worker_init_fn=worker_init_fn, generator=generator, # generator cho shuffle và worker_init_fn\n",
        "    num_workers=4, # Tăng num_workers nếu CPU/RAM cho phép để tăng tốc load dữ liệu\n",
        "    pin_memory=True # Giúp tăng tốc độ chuyển dữ liệu lên GPU\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "    worker_init_fn=worker_init_fn, # Đảm bảo tái lập kết quả\n",
        "    num_workers=4,\n",
        "    pin_memory=True\n",
        ")\n",
        "print(f\"Đã tạo DataLoader (batch_size={BATCH_SIZE})\")\n",
        "\n",
        "\n",
        "# 7. Train Model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"🔧 Sử dụng device: {device}\")\n",
        "\n",
        "model.to(device) # Chuyển mô hình sang thiết bị tính toán\n",
        "\n",
        "# Criterion (Hàm mất mát) và Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Sử dụng AdamW thay vì Adam thường để có khả năng điều chỉnh Weight Decay tốt hơn\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4) # Giảm LR và thêm Weight Decay\n",
        "\n",
        "# Learning Rate Scheduler (giúp cải thiện hiệu suất, đặc biệt với các mạng sâu)\n",
        "# Giảm learning rate khi validation loss ngừng cải thiện\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
        "\n",
        "NUM_EPOCHS = 25 # Tăng số epoch để ResNet18 có đủ thời gian học từ scratch\n",
        "print(f\"Bắt đầu huấn luyện ({NUM_EPOCHS} epochs)...\")\n",
        "\n",
        "history = {\n",
        "    'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'val_f1': []\n",
        "}\n",
        "best_val_f1 = -1.0\n",
        "best_epoch = -1\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    start_epoch_time = time.time()\n",
        "\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    train_loss = running_loss / total\n",
        "    train_acc = 100 * correct / total\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    val_correct, val_total = 0, 0\n",
        "    val_running_loss = 0.0\n",
        "    all_val_labels = []\n",
        "    all_val_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Val]\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "\n",
        "            all_val_labels.extend(labels.cpu().numpy())\n",
        "            all_val_preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "    val_loss = val_running_loss / val_total\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "\n",
        "    # Tính F1-score trên tập validation\n",
        "    # average='weighted' là lựa chọn tốt cho các tập dữ liệu không cân bằng\n",
        "    val_f1 = f1_score(all_val_labels, all_val_preds, average='weighted')\n",
        "\n",
        "    # Cập nhật Learning Rate Scheduler\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    history['val_f1'].append(val_f1)\n",
        "\n",
        "    epoch_duration = time.time() - start_epoch_time\n",
        "\n",
        "    print(f\"\\nEpoch [{epoch+1}/{NUM_EPOCHS}] ({epoch_duration:.2f}s):\")\n",
        "    print(f\" Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "    print(f\" Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, Val F1: {val_f1:.4f}\")\n",
        "\n",
        "    # Lưu model tốt nhất dựa trên Val F1-score\n",
        "    if val_f1 > best_val_f1:\n",
        "        best_val_f1 = val_f1\n",
        "        best_epoch = epoch + 1\n",
        "        model_path = os.path.join(root, \"resnet18_best_f1_model.pth\")\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "        print(f\"  >>> Lưu model tốt nhất với Val F1: {best_val_f1:.4f} tại Epoch {best_epoch} <<<\")\n",
        "\n",
        "print(\"\\nHoàn thành huấn luyện!\")\n",
        "print(f\"Model tốt nhất đạt Val F1 = {best_val_f1:.4f} tại Epoch {best_epoch}\")\n",
        "\n",
        "# 8. Lưu Model cuối cùng và lịch sử huấn luyện\n",
        "# (Chỉ lưu model tốt nhất trong vòng lặp)\n",
        "# model_path = os.path.join(root, \"resnet18_final_model.pth\")\n",
        "# torch.save(model.state_dict(), model_path)\n",
        "# print(f\"Đã lưu model cuối cùng tại: {model_path}\")\n",
        "\n",
        "history_path = os.path.join(root, \"training_history_resnet18.csv\")\n",
        "history_df = pd.DataFrame(history)\n",
        "history_df.to_csv(history_path, index=False)\n",
        "print(f\"Đã lưu lịch sử huấn luyện tại: {history_path}\")\n",
        "\n",
        "\n",
        "# --- Tải lại model tốt nhất để suy luận ---\n",
        "print(\"\\nĐang tải lại model tốt nhất để suy luận...\")\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.to(device)\n",
        "print(f\"Đã tải model từ '{model_path}'\")\n",
        "\n",
        "\n",
        "# 9. Inference trên Public Test Set\n",
        "print(\"\\nBắt đầu inference trên PUBLIC TEST SET...\")\n",
        "public_test_dataset = StormDataset(os.path.join(root, \"dataset/public_test\"), transform=transform, is_test=True)\n",
        "public_test_loader = DataLoader(\n",
        "    public_test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True\n",
        ")\n",
        "\n",
        "model.eval() # Chuyển model sang chế độ đánh giá\n",
        "file_names = []\n",
        "preds_is_negative = []\n",
        "preds_category_id = []\n",
        "\n",
        "with torch.no_grad(): # Không tính toán gradient trong quá trình suy luận\n",
        "    for i, (images, _) in enumerate(tqdm(public_test_loader, desc=\"Public Test\")):\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = outputs.max(1) # Lấy lớp có điểm số cao nhất\n",
        "\n",
        "        # Lấy tên file tương ứng từ DataFrame của Dataset\n",
        "        batch_start = i * BATCH_SIZE\n",
        "        batch_end = min(batch_start + images.size(0), len(public_test_dataset.annotations)) # Đảm bảo không vượt quá kích thước dataset\n",
        "        batch_files = public_test_dataset.annotations.iloc[batch_start:batch_end]['file_name'].values\n",
        "        file_names.extend(batch_files)\n",
        "\n",
        "        for p in predicted.cpu().numpy():\n",
        "            if p == 0: # Lớp 0 tương ứng với is_negative = True\n",
        "                preds_is_negative.append(True)\n",
        "                preds_category_id.append(\"\") # category_id để trống nếu is_negative là True\n",
        "            else: # Các lớp 1-4 tương ứng với category_id\n",
        "                preds_is_negative.append(False)\n",
        "                preds_category_id.append(int(p)) # Chuyển về int để lưu\n",
        "\n",
        "df_public = pd.DataFrame({\n",
        "    'file_name': file_names,\n",
        "    'is_negative': preds_is_negative,\n",
        "    'category_id': preds_category_id\n",
        "})\n",
        "\n",
        "public_csv_path = os.path.join(root, \"public_cv.csv\")\n",
        "df_public.to_csv(public_csv_path, index=False)\n",
        "print(f\"Đã tạo file public_cv.csv tại {public_csv_path} với {len(df_public)} dự đoán!\")\n",
        "print(\"\\nPreview kết quả Public Test:\")\n",
        "print(df_public.head(10))\n",
        "\n",
        "\n",
        "# 10. Inference trên Private Test Set\n",
        "print(\"\\nBắt đầu inference trên PRIVATE TEST SET...\")\n",
        "private_test_dataset = StormDataset(os.path.join(root, \"dataset/private_test\"), transform=transform, is_test=True)\n",
        "private_test_loader = DataLoader(\n",
        "    private_test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True\n",
        ")\n",
        "\n",
        "model.eval() # Chuyển model sang chế độ đánh giá\n",
        "file_names = []\n",
        "preds_is_negative = []\n",
        "preds_category_id = []\n",
        "\n",
        "with torch.no_grad(): # Không tính toán gradient trong quá trình suy luận\n",
        "    for i, (images, _) in enumerate(tqdm(private_test_loader, desc=\"Private Test\")):\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = outputs.max(1) # Lấy lớp có điểm số cao nhất\n",
        "\n",
        "        # Lấy tên file tương ứng từ DataFrame của Dataset\n",
        "        batch_start = i * BATCH_SIZE\n",
        "        batch_end = min(batch_start + images.size(0), len(private_test_dataset.annotations)) # Đảm bảo không vượt quá kích thước dataset\n",
        "        batch_files = private_test_dataset.annotations.iloc[batch_start:batch_end]['file_name'].values\n",
        "        file_names.extend(batch_files)\n",
        "\n",
        "        for p in predicted.cpu().numpy():\n",
        "            if p == 0: # Lớp 0 tương ứng với is_negative = True\n",
        "                preds_is_negative.append(True)\n",
        "                preds_category_id.append(\"\") # category_id để trống nếu is_negative là True\n",
        "            else: # Các lớp 1-4 tương ứng với category_id\n",
        "                preds_is_negative.append(False)\n",
        "                preds_category_id.append(int(p)) # Chuyển về int để lưu\n",
        "\n",
        "df_private = pd.DataFrame({\n",
        "    'file_name': file_names,\n",
        "    'is_negative': preds_is_negative,\n",
        "    'category_id': preds_category_id\n",
        "})\n",
        "\n",
        "private_csv_path = os.path.join(root, \"private_cv.csv\")\n",
        "df_private.to_csv(private_csv_path, index=False)\n",
        "print(f\"Đã tạo file private_cv.csv với {len(df_private)} dự đoán!\")\n",
        "print(\"\\nPreview kết quả Private Test:\")\n",
        "print(df_private.head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VA4x9bT5DXjQ",
        "outputId": "c42a0b55-819d-4e59-aee5-a6d9dfbb5b6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Đã thiết lập seed = 42\n",
            "Đã định nghĩa transform\n",
            "Đã định nghĩa StormDataset\n",
            "Đã xây dựng mô hình ResNet18\n",
            "Tổng số parameters: 11,179,077\n",
            "Tổng số ảnh huấn luyện/validation: 8385\n",
            "Tập huấn luyện: 6708, Tập validation: 1677\n",
            "Đã tạo DataLoader (batch_size=64)\n",
            "🔧 Sử dụng device: cuda\n",
            "Bắt đầu huấn luyện (25 epochs)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/25 [Train]:  57%|█████▋    | 60/105 [00:44<00:22,  2.02it/s]"
          ]
        }
      ]
    }
  ]
}